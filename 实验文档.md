### 实验文档

#### 1. 实验背景与目的

本实验旨在通过强化学习方法训练智能体在多智能体环境中进行决策和竞争。实验环境为3对3的贪吃蛇游戏，智能体通过不断学习和调整策略，在动态环境中获得优势。我们实现了基于深度确定性策略梯度（DDPG）算法的智能体，并在实验框架中与随机策略的智能体进行对比，以验证训练策略的有效性。

#### 2. 实验环境与设置

##### 2.1 环境说明

实验环境通过`SnakeEatBeans`类创建，环境初始化时从配置文件中读取地图大小、豆子数量等信息。主要功能包括：

- **状态初始化**：使用`env.reset()`方法初始化环境并获取智能体的初始观测数据。
- **动作决策**：红方和蓝方分别使用`red_policy`和`blue_policy`函数基于当前观测数据进行决策。
- **环境推演**：`env.step()`方法接受所有智能体的动作，并返回环境的下一状态、奖励、终止标志和其他信息。
- **游戏终局判断**：`env.check_win()`方法用于判断游戏的最终胜利方。

##### 2.2 实验框架

我们在测试时红色方选择采用随机策略的智能体（`red_policy`），我们实现了一个基于DDPG算法的智能体（`blue_policy`），用于测试和对比。

#### 3. 算法实现与说明

##### 3.1 DDPG算法简介

DDPG（深度确定性策略梯度）是一种基于Actor-Critic架构的强化学习算法。该算法使用演员网络（Actor）生成动作，使用评论家网络（Critic）评估动作的价值，通过经验回放和目标网络更新来稳定训练过程。

##### 3.2 实现细节

在`blue.py`文件中，我们实现了基于深度确定性策略梯度（DDPG）算法的智能体。

###### 1. 多层感知器（MLP）网络

**多层感知器（MLP）** 是深度学习中的基本结构，由多个全连接层（线性层）和激活函数组成。在我们的实现中，MLP网络用于构建Actor网络，这个网络的输入是智能体的观测数据，输出是动作的概率分布。

```python
def mlp(sizes, activation: Activation = 'relu', output_activation: Activation = 'identity'):
    if isinstance(activation, str):
        activation = _str_to_activation[activation]
    if isinstance(output_activation, str):
        output_activation = _str_to_activation[output_activation]

    layers = []
    for i in range(len(sizes) - 1):
        act = activation if i < len(sizes) - 2 else output_activation
        layers += [nn.Linear(sizes[i], sizes[i + 1]), act]
    return nn.Sequential(*layers)
```

- **输入参数**：`sizes` 是每一层的大小；`activation` 是隐藏层的激活函数，默认使用ReLU；`output_activation` 是输出层的激活函数，默认使用Identity。
- **工作原理**：根据输入的层大小和激活函数类型构建一个由线性层和激活函数交替组成的网络。

###### 2. 环境信息处理

环境信息处理模块用于从环境状态中提取与智能体决策相关的有用信息。在贪吃蛇游戏中，重要的信息包括蛇头的周围情况、食物的位置以及其他蛇的位置。

```python
def get_surrounding(state, width, height, x, y):
    surrounding = [state[(y - 1) % height][x],  # up
                   state[(y + 1) % height][x],  # down
                   state[y][(x - 1) % width],  # left
                   state[y][(x + 1) % width]]  # right

    return surrounding
```

- **功能**：获取蛇头上下左右四个方向的状态信息（如是否有障碍物或食物）。
- **用法**：在智能体决策时，调用此函数获取当前蛇头周围的环境信息，并将其作为输入传递给策略网络。

###### 3. 观察空间的提取

在多智能体环境中，每个智能体需要根据自身的观察来做出决策。`get_observations` 函数用于从环境状态中提取特定智能体的观测数据，包括自身位置、周围环境、食物位置以及其他蛇的位置。

```python
def get_observations(state, agents_index, obs_dim, height, width):
    ...
    for i, element in enumerate(agents_index):
        # 自己的头部位置
        observations[i][:2] = snakes_positions_list[element][0][:]
        ...
        # 头部周围的环境
        head_surrounding = get_surrounding(state_, width, height, head_x, head_y)
        observations[i][2:6] = head_surrounding[:]
        ...
        # 食物的位置
        observations[i][6:26] = beans_position[:]
        ...
        # 其他蛇的位置
        ...
    return observations
```

- **输入参数**：`state` 是环境的当前状态；`agents_index` 是智能体的索引；`obs_dim` 是观察维度；`height` 和 `width` 是游戏板的高度和宽度。
- **功能**：为每个智能体提取观测数据，包括自己的头部位置、头部周围的环境、食物位置和其他蛇的位置。这些信息组合成一个包含多个特征的观察向量。

###### 4. Actor类

**Actor类** 负责创建一个神经网络模型，该模型用于决定智能体在给定观测下的动作。这个模型通过多层感知器（MLP）实现，将观测数据映射到动作空间。

```python
class Actor(nn.Module):
    def __init__(self, obs_dim, act_dim, num_agents, args, output_activation='softmax'):
        ...
        self.prev_dense = mlp(sizes_prev)
        self.post_dense = mlp(sizes_post, output_activation=output_activation)

    def forward(self, obs_batch):
        out = self.prev_dense(obs_batch)
        out = self.post_dense(out)
        return out
```

- **输入参数**：`obs_dim` 是观测的维度；`act_dim` 是动作的维度；`num_agents` 是智能体的数量；`args` 是其他可选参数；`output_activation` 是输出层的激活函数，默认是Softmax。
- **功能**：将观测数据输入到前馈神经网络中，输出智能体在当前观测下应采取的动作概率分布。

###### 5. RLAgent类

**RLAgent类** 封装了DDPG的核心逻辑，包括动作选择、模型加载和策略执行。它通过调用Actor网络来决定智能体的动作，并将这些动作转换为环境能够理解的格式。

```python
class RLAgent(object):
    def __init__(self, obs_dim, act_dim, num_agent):
        ...
        self.actor = Actor(obs_dim, act_dim, num_agent, self.output_activation).to(self.device)

    def choose_action(self, obs):
        obs = torch.Tensor([obs]).to(self.device)
        logits = self.actor(obs).cpu().detach().numpy()[0]
        return logits

    def select_action_to_env(self, obs, ctrl_index):
        logits = self.choose_action(obs)
        actions = logits2action(logits)
        action_to_env = to_joint_action(actions, ctrl_index)
        return action_to_env

    def load_model(self, filename):
        self.actor.load_state_dict(torch.load(filename))
```

- **功能**：
  - **选择动作**：`choose_action` 方法输入观测数据，输出动作的概率分布（logits）。
  - **选择并转换动作**：`select_action_to_env` 方法选择动作并将其转换为环境能够接受的格式。
  - **加载模型**：`load_model` 方法加载预训练的Actor网络参数。

###### 6. 策略函数

**策略函数** 是智能体与环境交互的核心部分。在实验中调用`policy`函数来执行智能体的动作。该函数调用了`RLAgent`中的方法来选择和执行动作。

```python
def policy(observation_list):
    ...
    observation = get_observations(obs, indexs, obs_dim, height=board_height, width=board_width)
    actions = agent.select_action_to_env(observation, indexs.index(o_index-2))
    actions.append(action)
    return actions
```

- **功能**：根据输入的观测数据，调用`RLAgent`的方法生成智能体的动作，并返回这些动作以供环境执行。

##### 3.3 实验对比

在实验中，`red.py`中的随机策略智能体作为基准，`blue.py`中的DDPG智能体通过训练获得更优的策略。实验结果表明，经过训练的智能体在游戏中的表现显著优于随机策略。

#### 4. 使用方法

##### 4.1 训练模型

1. 运行以下命令以开始训练模型：
   ```bash
   python rl_trainer/main.py
   ```
2. 在训练过程中，模型将自动保存在`rl_trainer/models/snake_3v3`目录下，每次训练生成的模型保存在`run`文件夹中。

3. 训练过程中的参数可在`main.py`中修改，如训练轮数、学习率等。例如：
   ```python
   parser.add_argument('--max_episodes', default=400, type=int)
   ```
   你可以将`default=10`改为更大的值以增加训练轮数，从而提高模型的收敛性。

##### 4.2 测试训练好的模型

1. 训练完成后，将训练好的模型文件（例如`actor_10000.pth`）复制到`submissions`目录。
2. 修改`blue.py`中的对应路径，确保加载正确的模型文件：
   ```python
   actor_net = os.path.dirname(os.path.abspath(__file__)) + "/actor_10000.pth"
   ```
3. 运行以下命令以测试模型：
   ```bash
   python example.py
   ```
4. 运行结果将输出智能体的胜负情况。

##### 4.3 继续训练已有模型

1. 如果希望在已有模型的基础上继续训练，可以在`main.py`中修改以下参数：
   - `load_model`：设置为`True`以加载已有模型。
   - `load_model_run`：指定加载哪个`run`文件夹的模型。
   - `load_model_run_episode`：指定从该文件夹内的哪一轮模型继续训练。

2. 例如，继续训练`run4`文件夹下第5轮的模型：
   ```python
   load_model = True
   load_model_run = 4
   load_model_run_episode = 5
   ```
